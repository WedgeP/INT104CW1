{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Task 1: Response to Data analysis and feature engineering\n",
    "使用pandas导入训练数据集，并分析特征和标签的分布情况。"
   ],
   "id": "204c0754e749b3d5"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import platform\n",
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "from sklearn.decomposition import PCA, FastICA, TruncatedSVD\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "\n",
    "# 设置随机种子和中文显示\n",
    "np.random.seed(42)\n",
    "system = platform.system()\n",
    "if system == 'Darwin':\n",
    "    plt.rcParams['font.sans-serif'] = ['Arial Unicode MS']\n",
    "elif system == 'Windows':\n",
    "    plt.rcParams['font.sans-serif'] = ['SimHei']\n",
    "elif system == 'Linux':\n",
    "    plt.rcParams['font.sans-serif'] = ['Noto Sans CJK SC']\n",
    "else:\n",
    "    plt.rcParams['font.sans-serif'] = ['SimHei']\n",
    "\n",
    "df = pd.read_csv('./student_data.csv')\n",
    "\n",
    "grade_models = {\n",
    "    'pca_model': {},\n",
    "    'bayes_models': {},\n",
    "    'random_forest_models': {}\n",
    "}\n",
    "\n",
    "mapping = {1: 'A', 2: 'B', 3: 'C', 4: 'D'}\n",
    "if df['Programme'].dtype == 'int64' or df['Programme'].iloc[0] in [1, 2, 3, 4]:\n",
    "    df['Programme'] = df['Programme'].map(mapping)\n",
    "\n",
    "def process_data(df, mode='train', preprocessors=None):\n",
    "    if 'Index' in df.columns:\n",
    "        df = df.drop('Index', axis=1)\n",
    "    numeric_df = df.select_dtypes(include=['float64', 'int64'])\n",
    "    feature_sets = {}\n",
    "\n",
    "    if mode == 'train':\n",
    "        preprocessors = {'scalers': {}, 'columns': {}}\n",
    "\n",
    "    # 1. 考试分数\n",
    "    if mode == 'train':\n",
    "        exam_cols = [col for col in numeric_df.columns if 'Q' in col]\n",
    "        if not exam_cols:\n",
    "            exam_cols = numeric_df.columns[-5:].tolist()\n",
    "        preprocessors['columns']['考试分数'] = exam_cols\n",
    "    else:\n",
    "        exam_cols = preprocessors['columns']['考试分数']\n",
    "    # 补齐缺失列并保证顺序\n",
    "    for col in exam_cols:\n",
    "        if col not in numeric_df.columns:\n",
    "            numeric_df[col] = 0\n",
    "    feature_sets['考试分数'] = numeric_df[exam_cols].values\n",
    "\n",
    "    # 2. 基本信息\n",
    "    basic_patterns = ['性别', 'Gender', 'sex', 'Total', '总分', 'Q1', 'Q2', 'Q3', 'Q4', 'Q5']\n",
    "    if mode == 'train':\n",
    "        basic_cols = []\n",
    "        for pat in basic_patterns:\n",
    "            basic_cols += [col for col in numeric_df.columns if pat.lower() in col.lower()]\n",
    "        basic_cols = list(dict.fromkeys(basic_cols))\n",
    "        if not basic_cols:\n",
    "            basic_cols = numeric_df.columns[:2].tolist()\n",
    "        preprocessors['columns']['去除年级'] = basic_cols\n",
    "    else:\n",
    "        basic_cols = preprocessors['columns']['去除年级']\n",
    "    for col in basic_cols:\n",
    "        if col not in numeric_df.columns:\n",
    "            numeric_df[col] = 0\n",
    "    feature_sets['去除年级'] = numeric_df[basic_cols].values\n",
    "\n",
    "    # 3. 全部特征（排除programme列）\n",
    "    if mode == 'train':\n",
    "        programme_cols = [col for col in numeric_df.columns if 'programme' in col.lower() or 'program' in col.lower()]\n",
    "        all_cols = [col for col in numeric_df.columns if col not in programme_cols]\n",
    "        preprocessors['columns']['全部特征'] = all_cols\n",
    "    else:\n",
    "        all_cols = preprocessors['columns']['全部特征']\n",
    "    for col in all_cols:\n",
    "        if col not in numeric_df.columns:\n",
    "            numeric_df[col] = 0\n",
    "    feature_sets['全部特征'] = numeric_df[all_cols].values\n",
    "\n",
    "    # 标准化\n",
    "    for name, data in feature_sets.items():\n",
    "        if mode == 'train':\n",
    "            scaler = StandardScaler()\n",
    "            feature_sets[name] = scaler.fit_transform(data)\n",
    "            preprocessors['scalers'][name] = scaler\n",
    "        else:\n",
    "            feature_sets[name] = preprocessors['scalers'][name].transform(data)\n",
    "\n",
    "    if mode == 'train':\n",
    "        return feature_sets, preprocessors\n",
    "    else:\n",
    "        return feature_sets"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "导入数据后，查看数据集的基本信息。处理数据缺失\n",
    "## 特征转换\n",
    "应用三种不同的数据转换方法：标准化缩放、PCA降维和独立成分分析(ICA)"
   ],
   "id": "463a157097939261"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "# 去除索引列（如果存在）\n",
    "if 'Index' in df.columns:\n",
    "    df = df.drop('Index', axis=1)\n",
    "\n",
    "print(f\"数据集形状: {df.shape}\")\n",
    "print(\"\\n数据集前5行:\")\n",
    "display(df.head())\n",
    "print(\"\\n数据集信息:\")\n",
    "display(df.info())\n",
    "print(f\"\\n缺失值情况:\")\n",
    "display(df.isnull().sum())\n",
    "\n",
    "# 只保留数值特征\n",
    "numeric_df = df.select_dtypes(include=['float64', 'int64'])\n",
    "feature_sets, preprocessors = process_data(df, mode='train')\n",
    "\n",
    "print(\"创建的特征集:\")\n",
    "for name, features in feature_sets.items():\n",
    "    print(f\"- {name}: 形状 {features.shape}\")\n",
    "\n",
    "# 检查特征集并标准化\n",
    "for name, data in feature_sets.items():\n",
    "    print(f\"\\n特征集: {name}\")\n",
    "    print(f\"数据集形状: {data.shape}\")\n",
    "    print(\"前5行:\")\n",
    "    print(pd.DataFrame(data[:5]))\n",
    "    # 标准化\n",
    "    scaler = StandardScaler()\n",
    "    feature_sets[name] = scaler.fit_transform(data)\n",
    "print(\"\\n数据集统计描述:\")\n",
    "display(df.describe())"
   ],
   "id": "1cd20b2f5cc4bf8d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 特征转换\n",
    "应用三种不同的数据转换方法：标准化缩放、PCA降维和独立成分分析(ICA)"
   ],
   "id": "80e120ce89748023"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "feature_set = feature_sets\n",
    "feature_sets = {}\n",
    "for name, X in feature_set.items():\n",
    "   # 转换1: 归一化 (MinMaxScaler)\n",
    "    min_max_scaler = MinMaxScaler()\n",
    "    X_minmax = min_max_scaler.fit_transform(X)\n",
    "    feature_sets['归一化_' + name] = X_minmax\n",
    "    print(f\"{name} 已完成归一化转换\")\n",
    "\n",
    "    # 转换2: 标准化 (StandardScaler)\n",
    "    std_scaler = StandardScaler()\n",
    "    X_std = std_scaler.fit_transform(X)\n",
    "    feature_sets['标准化_' + name] = X_std\n",
    "    print(f\"{name} 已完成标准化转换\")\n",
    "\n",
    "    normalizer = Normalizer()\n",
    "    X_normalized = normalizer.fit_transform(X)\n",
    "    feature_sets['正则缩放_' + name] = X_normalized\n",
    "    print(f\"{name} 已完成正则缩放转换\")\n",
    "\n",
    "feature_set = feature_sets\n",
    "feature_sets = {}\n",
    "\n",
    "for name, X_scaled in feature_set.items():\n",
    "    # 转换2: PCA降维\n",
    "    pca = PCA(n_components=min(X_scaled.shape[1], 10))\n",
    "    X_pca = pca.fit_transform(X_scaled)\n",
    "    feature_sets['PCA_' + name] = X_pca\n",
    "    print(f\"{name} PCA解释方差比: {pca.explained_variance_ratio_}\")\n",
    "    print(f\"{name} PCA累计方差占比: {np.sum(pca.explained_variance_ratio_):.4f}\")\n",
    "\n",
    "    # 转换3: FastICA\n",
    "    ica = FastICA(n_components=min(X_scaled.shape[1], 10), random_state=42)\n",
    "    X_ica = ica.fit_transform(X_scaled)\n",
    "    feature_sets['ICA_' + name] = X_ica\n",
    "    print(f\"{name} 已完成ICA转换\")\n",
    "    # 转换4: t-SNE\n",
    "    tsne = TSNE(n_components=2, random_state=42, init='random', learning_rate='auto')\n",
    "    X_tsne = tsne.fit_transform(X_scaled)\n",
    "    feature_sets['TSNE_' + name] = X_tsne\n",
    "    print(f\"{name} 已完成t-SNE转换\")\n",
    "\n",
    "# 可视化所有特征集\n",
    "n = len(feature_sets)\n",
    "cols = 3\n",
    "rows = math.ceil(n / cols)\n",
    "plt.figure(figsize=(5 * cols, 5 * rows))\n",
    "for i, (fname, data) in enumerate(feature_sets.items()):\n",
    "    plt.subplot(rows, cols, i + 1)\n",
    "    if data.shape[1] > 1:\n",
    "        plt.scatter(data[:, 0], data[:, 1], alpha=0.5)\n",
    "        plt.title(f\"{fname} (前两个维度)\")\n",
    "    else:\n",
    "        plt.hist(data[:, 0], bins=20)\n",
    "        plt.title(f\"{fname}分布\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "58371bedbe221148",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 聚类评估函数\n",
    "定义用于评估聚类结果的性能指标函数"
   ],
   "id": "1e252efeb4b9f3f6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 第四个代码单元格 - 评估函数\n",
    "def evaluate_clustering(X, labels, name):\n",
    "    \"\"\"计算聚类性能指标\"\"\"\n",
    "    try:\n",
    "        silhouette = silhouette_score(X, labels)\n",
    "    except:\n",
    "        silhouette = -1\n",
    "\n",
    "    try:\n",
    "        db_score = davies_bouldin_score(X, labels)\n",
    "    except:\n",
    "        db_score = float('inf')\n",
    "\n",
    "    try:\n",
    "        ch_score = calinski_harabasz_score(X, labels)\n",
    "    except:\n",
    "        ch_score = -1\n",
    "\n",
    "    return {\n",
    "        'silhouette_score': silhouette,  # 越高越好\n",
    "        'davies_bouldin_score': db_score,  # 越低越好\n",
    "        'calinski_harabasz_score': ch_score,  # 越高越好\n",
    "        'method': name\n",
    "    }"
   ],
   "id": "22122caea0e5c02b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 聚类算法实现\n",
    "实现三种聚类算法及其不同参数设置：\n",
    "1. K-means聚类\n",
    "2. 高斯混合模型(GMM)\n",
    "3. 层次聚类(Hierarchical Clustering)"
   ],
   "id": "912161e51de0a99"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def run_kmeans(X, n_clusters_range=[4], init_methods=['k-means++', 'random']):\n",
    "    \"\"\"运行K-means并尝试不同参数\"\"\"\n",
    "    results = []\n",
    "\n",
    "    for n_clusters in n_clusters_range:\n",
    "        for init in init_methods:\n",
    "            name = f\"KMeans(n_clusters={n_clusters}, init={init})\"\n",
    "            try:\n",
    "                model = KMeans(n_clusters=n_clusters, init=init, random_state=42)\n",
    "                labels = model.fit_predict(X)\n",
    "\n",
    "                # 评估结果\n",
    "                result = evaluate_clustering(X, labels, name)\n",
    "                result['labels'] = labels\n",
    "                result['model'] = model\n",
    "                result['n_clusters'] = n_clusters\n",
    "                result['init'] = init\n",
    "\n",
    "                if hasattr(X, 'columns'):  # 如果X是DataFrame\n",
    "                    result['feature_names'] = X.columns.tolist()\n",
    "                else:  # 如果X是ndarray（比如预处理后）\n",
    "                    result['feature_names'] = [f\"feature_{i}\" for i in range(X.shape[1])]\n",
    "\n",
    "                results.append(result)\n",
    "                print(f\"完成: {name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"错误 {name}: {str(e)}\")\n",
    "\n",
    "    return results"
   ],
   "id": "fb3cd5861694686f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def run_gmm(X, n_components_range=[2, 3, 4, 5, 6, 7, 8], covariance_types=['full', 'tied', 'diag', 'spherical']):\n",
    "    \"\"\"运行高斯混合模型并尝试不同参数\"\"\"\n",
    "    results = []\n",
    "\n",
    "    for n_components in n_components_range:\n",
    "        for cov_type in covariance_types:\n",
    "            name = f\"GMM(n_components={n_components}, covariance_type={cov_type})\"\n",
    "            try:\n",
    "                model = GaussianMixture(n_components=n_components, covariance_type=cov_type, random_state=42)\n",
    "                labels = model.fit_predict(X)\n",
    "\n",
    "                # 评估结果\n",
    "                result = evaluate_clustering(X, labels, name)\n",
    "                result['labels'] = labels\n",
    "                result['model'] = model\n",
    "                result['n_components'] = n_components\n",
    "                result['covariance_type'] = cov_type\n",
    "\n",
    "                if hasattr(X, 'columns'):  # 如果是DataFrame\n",
    "                    result['feature_names'] = X.columns.tolist()\n",
    "                else:  # 如果是ndarray\n",
    "                    result['feature_names'] = [f\"feature_{i}\" for i in range(X.shape[1])]\n",
    "\n",
    "                results.append(result)\n",
    "                print(f\"完成: {name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"错误 {name}: {str(e)}\")\n",
    "\n",
    "    return results"
   ],
   "id": "810332aa41d28560",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def run_hierarchical(X, n_clusters_range=[4], linkage_methods=['ward', 'complete', 'average', 'single']):\n",
    "    \"\"\"运行层次聚类并尝试不同参数\"\"\"\n",
    "    results = []\n",
    "\n",
    "    for n_clusters in n_clusters_range:\n",
    "        for linkage in linkage_methods:\n",
    "            # ward 只能用于欧氏距离\n",
    "            if linkage == 'ward':\n",
    "                metric = 'euclidean'\n",
    "            else:\n",
    "                metric = 'euclidean'  # 也可以尝试其他距离，比如 'manhattan'\n",
    "\n",
    "            name = f\"HC(n_clusters={n_clusters}, linkage={linkage})\"\n",
    "            try:\n",
    "                model = AgglomerativeClustering(\n",
    "                    n_clusters=n_clusters,\n",
    "                    linkage=linkage,\n",
    "                    metric=metric  # metric 是新版 sklearn 的参数，老版叫 affinity\n",
    "                )\n",
    "                labels = model.fit_predict(X)\n",
    "\n",
    "                # 评估结果\n",
    "                result = evaluate_clustering(X, labels, name)\n",
    "                result['labels'] = labels\n",
    "                result['model'] = model\n",
    "                result['n_clusters'] = n_clusters\n",
    "                result['linkage'] = linkage\n",
    "\n",
    "                # 补充 feature_names\n",
    "                if hasattr(X, 'columns'):  # DataFrame\n",
    "                    result['feature_names'] = X.columns.tolist()\n",
    "                else:  # Numpy array\n",
    "                    result['feature_names'] = [f\"feature_{i}\" for i in range(X.shape[1])]\n",
    "\n",
    "                results.append(result)\n",
    "                print(f\"完成: {name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"错误 {name}: {str(e)}\")\n",
    "\n",
    "    return results"
   ],
   "id": "dd71590f41beeb47",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 运行聚类实验\n",
    "对每种特征集运行三种聚类算法，并尝试不同的参数设置"
   ],
   "id": "e1ae4fa3230ca7aa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 第八个代码单元格 - 运行所有实验\n",
    "# 为了限制运行时间，可以减少参数组合\n",
    "n_clusters_range = [4]  # 聚类数量范围\n",
    "init_methods =['k-means++']  # K-means初始化方法\n",
    "covariance_types = ['full', 'tied', 'diag', 'spherical']  # GMM协方差类型\n",
    "linkage_methods = ['ward', 'complete', 'average', 'single'] # 层次聚类链接方法\n",
    "\n",
    "# 4. 运行所有实验并收集结果\n",
    "all_results = {}\n",
    "\n",
    "for feature_name, X_transformed in feature_sets.items():\n",
    "    print(f\"\\n处理特征集: {feature_name}\")\n",
    "\n",
    "    # 运行三种聚类算法\n",
    "    kmeans_results = run_kmeans(X_transformed, n_clusters_range, init_methods)\n",
    "    gmm_results = run_gmm(X_transformed, n_clusters_range, covariance_types)\n",
    "    hc_results = run_hierarchical(X_transformed, n_clusters_range, linkage_methods)\n",
    "\n",
    "    # 保存结果\n",
    "    all_results[feature_name] = {\n",
    "        'kmeans': kmeans_results,\n",
    "        'gmm': gmm_results,\n",
    "        'hierarchical': hc_results\n",
    "    }"
   ],
   "id": "2135d667c47b14a4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 结果分析\n",
    "找出每种特征集和每种聚类方法的最佳结果"
   ],
   "id": "52058251351e3093"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 第九个代码单元格 - 找出最佳结果\n",
    "# 5. 找出每种特征集和每种聚类方法的最佳结果\n",
    "best_results = {}\n",
    "\n",
    "for feature_name, methods in all_results.items():\n",
    "    best_results[feature_name] = {}\n",
    "\n",
    "    for method_name, results in methods.items():\n",
    "        if method_name in ['kmeans', 'gmm', 'hierarchical']:\n",
    "            # 按silhouette_score排序（越高越好）\n",
    "            sorted_results = sorted(results, key=lambda x: x['silhouette_score'], reverse=True)\n",
    "            if sorted_results:\n",
    "                best_results[feature_name][method_name] = sorted_results[0]"
   ],
   "id": "331186d01fb8850b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 第十个代码单元格 - 创建结果表格\n",
    "# 6. 创建结果表格\n",
    "results_table = []\n",
    "\n",
    "for feature_name, methods in best_results.items():\n",
    "    for method_name, result in methods.items():\n",
    "        row = {\n",
    "            '特征集': feature_name,\n",
    "            '聚类方法': method_name,\n",
    "            '轮廓系数': result['silhouette_score'],\n",
    "            'Davies-Bouldin': result['davies_bouldin_score'],\n",
    "            'Calinski-Harabasz': result['calinski_harabasz_score']\n",
    "        }\n",
    "\n",
    "        # 添加模型特有的参数\n",
    "        if method_name == 'kmeans':\n",
    "            row['聚类数'] = result['n_clusters']\n",
    "            row['初始化方法'] = result['init']\n",
    "        elif method_name == 'gmm':\n",
    "            row['聚类数/组件数'] = result['n_components']\n",
    "            row['协方差类型'] = result['covariance_type']\n",
    "        elif method_name == 'hierarchical':\n",
    "            row['聚类数'] = result['n_clusters']\n",
    "            row['链接方法'] = result['linkage']\n",
    "\n",
    "        results_table.append(row)\n",
    "\n",
    "# 7. 将结果转换为DataFrame并显示\n",
    "results_df = pd.DataFrame(results_table)\n",
    "print(\"\\n聚类结果表:\")\n",
    "display(results_df)"
   ],
   "id": "d0dc20f3b3c904bd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 可视化聚类结果\n",
    "可视化展示每种特征集和聚类方法的最佳聚类结果"
   ],
   "id": "cc05aed3c300399a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 第十一个代码单元格 - 可视化最佳结果\n",
    "# 8. 可视化每种特征集和聚类方法的最佳结果\n",
    "for feature_name, methods in best_results.items():\n",
    "    for method_name, result in methods.items():\n",
    "        # 如果特征维度大于2，使用PCA降至2维进行可视化\n",
    "        if feature_sets[feature_name].shape[1] > 2:\n",
    "            vis_pca = PCA(n_components=2)\n",
    "            X_vis = vis_pca.fit_transform(feature_sets[feature_name])\n",
    "        else:\n",
    "            X_vis = feature_sets[feature_name]\n",
    "\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.scatter(X_vis[:, 0], X_vis[:, 1], c=result['labels'], cmap='viridis', alpha=0.8, s=50)\n",
    "        plt.title(f'特征集: {feature_name}, 聚类方法: {method_name}\\n轮廓系数: {result[\"silhouette_score\"]:.4f}')\n",
    "        plt.colorbar(label='聚类标签')\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ],
   "id": "40f5e316f7a0b8b3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 第十二个代码单元格 - 汇总表格\n",
    "# 9. 生成最佳结果的汇总表格\n",
    "summary_df = pd.DataFrame(results_table)\n",
    "\n",
    "# 按特征集和聚类方法分组，找出每种组合的最佳结果\n",
    "best_by_feature = summary_df.sort_values('轮廓系数', ascending=False).groupby(['特征集', '聚类方法']).first().reset_index()\n",
    "\n",
    "print(\"最佳聚类结果汇总表:\")\n",
    "display(best_by_feature)\n",
    "\n",
    "# 创建热力图显示不同特征集和聚类方法的轮廓系数\n",
    "pivot_table = best_by_feature.pivot(index='特征集', columns='聚类方法', values='轮廓系数')\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(pivot_table, annot=True, cmap='YlGnBu', fmt='.4f')\n",
    "plt.title('不同特征集和聚类方法的轮廓系数')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "5398d57f7fec47cf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import warnings\n",
    "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score, confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 真实标签编码，只需一次\n",
    "y_true = LabelEncoder().fit_transform(df['Programme'])\n",
    "\n",
    "# 结果表格\n",
    "results_table = []\n",
    "\n",
    "for feature_name, methods in best_results.items():\n",
    "    for method_name, result in methods.items():\n",
    "        # 记录聚类评估指标\n",
    "        row = {\n",
    "            '特征集': feature_name,\n",
    "            '聚类方法': method_name,\n",
    "            '轮廓系数': result['silhouette_score'],\n",
    "            'Davies-Bouldin': result['davies_bouldin_score'],\n",
    "            'Calinski-Harabasz': result['calinski_harabasz_score']\n",
    "        }\n",
    "        if method_name == 'kmeans':\n",
    "            row['聚类数'] = result['n_clusters']\n",
    "            row['初始化方法'] = result['init']\n",
    "        elif method_name == 'gmm':\n",
    "            row['组件数'] = result['n_components']\n",
    "            row['协方差类型'] = result['covariance_type']\n",
    "        elif method_name == 'hierarchical':\n",
    "            row['聚类数'] = result['n_clusters']\n",
    "            row['链接方法'] = result['linkage']\n",
    "        results_table.append(row)\n",
    "\n",
    "        # 可视化\n",
    "        X_plot = feature_sets[feature_name]\n",
    "        if X_plot.shape[1] > 2:\n",
    "            X_vis = PCA(n_components=2).fit_transform(X_plot)\n",
    "        else:\n",
    "            X_vis = X_plot\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.scatter(X_vis[:, 0], X_vis[:, 1], c=result['labels'], cmap='viridis', alpha=0.8, s=50)\n",
    "        plt.title(f'特征集: {feature_name}, 聚类方法: {method_name}\\n轮廓系数: {result[\"silhouette_score\"]:.4f}')\n",
    "        plt.colorbar(label='聚类标签')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # 聚类与真实标签对比\n",
    "        labels = result['labels']\n",
    "        ari = adjusted_rand_score(y_true, labels)\n",
    "        nmi = normalized_mutual_info_score(y_true, labels)\n",
    "        print(f\"\\n特征集: {feature_name}, 聚类方法: {method_name}\")\n",
    "        print(f\"调整兰德指数(ARI): {ari:.4f}\")\n",
    "        print(f\"归一化互信息(NMI): {nmi:.4f}\")\n",
    "        print(\"混淆矩阵:\")\n",
    "        print(confusion_matrix(y_true, labels))\n",
    "\n",
    "# 结果表格展示\n",
    "results_df = pd.DataFrame(results_table)\n",
    "print(\"\\n聚类结果表:\")\n",
    "print(results_df)"
   ],
   "id": "c9d7bda69195b35e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-26T11:36:40.356938Z",
     "start_time": "2025-04-26T11:36:40.123133Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 1. 找到轮廓系数最高的模型\n",
    "best_model = None\n",
    "best_score = -float('inf')\n",
    "best_feature_name = None\n",
    "best_method = None\n",
    "test_df = pd.read_csv(\"./student_data.csv\")\n",
    "\n",
    "for feature_name, methods in all_results.items():\n",
    "    for method_name, results in methods.items():\n",
    "        for result in results:\n",
    "            if result['silhouette_score'] > best_score:\n",
    "                best_score = result['silhouette_score']\n",
    "                best_model = result['model']\n",
    "                best_feature_name = result['feature_names']\n",
    "                best_method = method_name\n",
    "\n",
    "print(f\"最佳模型: 特征集={best_feature_name}, 方法={best_method}, 轮廓系数={best_score:.4f}\")\n",
    "\n",
    "# 2. 用最佳特征集的scaler和特征处理测试集\n",
    "test_feature_sets = process_data(test_df, mode='test', preprocessors=preprocessors)\n",
    "\n",
    "# ✅ 直接用完整的 feature 名字\n",
    "X_test_scaled = test_feature_sets[best_feature_name]\n",
    "\n",
    "# 3. 用最佳模型预测\n",
    "test_labels = best_model.predict(X_test_scaled)\n",
    "\n",
    "# 4. 评估\n",
    "result = evaluate_clustering(X=X_test_scaled, labels=test_labels, name='best_model')\n",
    "print(f\"轮廓系数: {result['silhouette_score']:.4f}\")"
   ],
   "id": "f13bcab11e7f00bd",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "最佳模型: 特征集=['feature_0', 'feature_1'], 方法=kmeans, 轮廓系数=0.5568\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'feature_0'",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mValueError\u001B[39m                                Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[102]\u001B[39m\u001B[32m, line 26\u001B[39m\n\u001B[32m     23\u001B[39m X_test_scaled = best_feature_name\n\u001B[32m     25\u001B[39m \u001B[38;5;66;03m# 3. 用最佳模型预测\u001B[39;00m\n\u001B[32m---> \u001B[39m\u001B[32m26\u001B[39m test_labels = best_model.predict(X_test_scaled)\n\u001B[32m     28\u001B[39m \u001B[38;5;66;03m# 4. 评估\u001B[39;00m\n\u001B[32m     29\u001B[39m result = evaluate_clustering(X=X_test_scaled, labels=test_labels, name=\u001B[33m'\u001B[39m\u001B[33mbest_model\u001B[39m\u001B[33m'\u001B[39m)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/anaconda3/envs/INT104CW1/lib/python3.12/site-packages/sklearn/cluster/_kmeans.py:1085\u001B[39m, in \u001B[36m_BaseKMeans.predict\u001B[39m\u001B[34m(self, X)\u001B[39m\n\u001B[32m   1067\u001B[39m \u001B[38;5;250m\u001B[39m\u001B[33;03m\"\"\"Predict the closest cluster each sample in X belongs to.\u001B[39;00m\n\u001B[32m   1068\u001B[39m \n\u001B[32m   1069\u001B[39m \u001B[33;03mIn the vector quantization literature, `cluster_centers_` is called\u001B[39;00m\n\u001B[32m   (...)\u001B[39m\u001B[32m   1081\u001B[39m \u001B[33;03m    Index of the cluster each sample belongs to.\u001B[39;00m\n\u001B[32m   1082\u001B[39m \u001B[33;03m\"\"\"\u001B[39;00m\n\u001B[32m   1083\u001B[39m check_is_fitted(\u001B[38;5;28mself\u001B[39m)\n\u001B[32m-> \u001B[39m\u001B[32m1085\u001B[39m X = \u001B[38;5;28mself\u001B[39m._check_test_data(X)\n\u001B[32m   1087\u001B[39m \u001B[38;5;66;03m# sample weights are not used by predict but cython helpers expect an array\u001B[39;00m\n\u001B[32m   1088\u001B[39m sample_weight = np.ones(X.shape[\u001B[32m0\u001B[39m], dtype=X.dtype)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/anaconda3/envs/INT104CW1/lib/python3.12/site-packages/sklearn/cluster/_kmeans.py:944\u001B[39m, in \u001B[36m_BaseKMeans._check_test_data\u001B[39m\u001B[34m(self, X)\u001B[39m\n\u001B[32m    943\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34m_check_test_data\u001B[39m(\u001B[38;5;28mself\u001B[39m, X):\n\u001B[32m--> \u001B[39m\u001B[32m944\u001B[39m     X = validate_data(\n\u001B[32m    945\u001B[39m         \u001B[38;5;28mself\u001B[39m,\n\u001B[32m    946\u001B[39m         X,\n\u001B[32m    947\u001B[39m         accept_sparse=\u001B[33m\"\u001B[39m\u001B[33mcsr\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m    948\u001B[39m         reset=\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[32m    949\u001B[39m         dtype=[np.float64, np.float32],\n\u001B[32m    950\u001B[39m         order=\u001B[33m\"\u001B[39m\u001B[33mC\u001B[39m\u001B[33m\"\u001B[39m,\n\u001B[32m    951\u001B[39m         accept_large_sparse=\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[32m    952\u001B[39m     )\n\u001B[32m    953\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m X\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/anaconda3/envs/INT104CW1/lib/python3.12/site-packages/sklearn/utils/validation.py:2944\u001B[39m, in \u001B[36mvalidate_data\u001B[39m\u001B[34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001B[39m\n\u001B[32m   2942\u001B[39m         out = X, y\n\u001B[32m   2943\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m no_val_X \u001B[38;5;129;01mand\u001B[39;00m no_val_y:\n\u001B[32m-> \u001B[39m\u001B[32m2944\u001B[39m     out = check_array(X, input_name=\u001B[33m\"\u001B[39m\u001B[33mX\u001B[39m\u001B[33m\"\u001B[39m, **check_params)\n\u001B[32m   2945\u001B[39m \u001B[38;5;28;01melif\u001B[39;00m no_val_X \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m no_val_y:\n\u001B[32m   2946\u001B[39m     out = _check_y(y, **check_params)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/anaconda3/envs/INT104CW1/lib/python3.12/site-packages/sklearn/utils/validation.py:1055\u001B[39m, in \u001B[36mcheck_array\u001B[39m\u001B[34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001B[39m\n\u001B[32m   1053\u001B[39m         array = xp.astype(array, dtype, copy=\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[32m   1054\u001B[39m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1055\u001B[39m         array = _asarray_with_order(array, order=order, dtype=dtype, xp=xp)\n\u001B[32m   1056\u001B[39m \u001B[38;5;28;01mexcept\u001B[39;00m ComplexWarning \u001B[38;5;28;01mas\u001B[39;00m complex_warning:\n\u001B[32m   1057\u001B[39m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[32m   1058\u001B[39m         \u001B[33m\"\u001B[39m\u001B[33mComplex data not supported\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{}\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[33m\"\u001B[39m.format(array)\n\u001B[32m   1059\u001B[39m     ) \u001B[38;5;28;01mfrom\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34;01mcomplex_warning\u001B[39;00m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m/opt/anaconda3/envs/INT104CW1/lib/python3.12/site-packages/sklearn/utils/_array_api.py:839\u001B[39m, in \u001B[36m_asarray_with_order\u001B[39m\u001B[34m(array, dtype, order, copy, xp, device)\u001B[39m\n\u001B[32m    837\u001B[39m     array = numpy.array(array, order=order, dtype=dtype)\n\u001B[32m    838\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m--> \u001B[39m\u001B[32m839\u001B[39m     array = numpy.asarray(array, order=order, dtype=dtype)\n\u001B[32m    841\u001B[39m \u001B[38;5;66;03m# At this point array is a NumPy ndarray. We convert it to an array\u001B[39;00m\n\u001B[32m    842\u001B[39m \u001B[38;5;66;03m# container that is consistent with the input's namespace.\u001B[39;00m\n\u001B[32m    843\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m xp.asarray(array)\n",
      "\u001B[31mValueError\u001B[39m: could not convert string to float: 'feature_0'"
     ]
    }
   ],
   "execution_count": 102
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
