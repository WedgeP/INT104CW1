{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Task 1: Response to Data analysis and feature engineering\n",
    "使用pandas导入训练数据集，并分析特征和标签的分布情况。"
   ],
   "id": "204c0754e749b3d5"
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import platform\n",
    "import math\n",
    "# 导入必要的库\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "# 导入缺失的库\n",
    "from sklearn.decomposition import FastICA\n",
    "import seaborn as sns\n",
    "# 导入聚类相关的库\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "import seaborn as sns\n",
    "from sklearn.decomposition import FastICA, TruncatedSVD\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import warnings\n",
    "\n",
    "\n",
    "\n",
    "# 设置随机种子和中文显示\n",
    "np.random.seed(42)\n",
    "# 设置中文显示\n",
    "system = platform.system()\n",
    "\n",
    "if system == 'Darwin':  # Mac系统\n",
    "    plt.rcParams['font.sans-serif'] = ['Arial Unicode MS']\n",
    "elif system == 'Windows':  # Windows系统\n",
    "    plt.rcParams['font.sans-serif'] = ['SimHei']  # 黑体\n",
    "elif system == 'Linux':  # Linux系统\n",
    "    # Linux系统可能需要安装中文字体，例如 Noto Sans CJK SC\n",
    "    plt.rcParams['font.sans-serif'] = ['Noto Sans CJK SC']\n",
    "else:\n",
    "    # 默认字体（如果系统未识别）\n",
    "    plt.rcParams['font.sans-serif'] = ['SimHei']\n",
    "\n",
    "# 读取数据\n",
    "df = pd.read_csv(\"./student_data.csv\")\n",
    "\n",
    "# 初始化模型存储结构\n",
    "# 在代码开头初始化完整的字典结构\n",
    "grade_models = {\n",
    "    'pca_model': {},\n",
    "    'bayes_models': {},\n",
    "    'random_forest_models': {}\n",
    "}\n",
    "\n",
    "# 系别映射（如果需要）\n",
    "mapping = {1: 'A', 2: 'B', 3: 'C', 4: 'D'}\n",
    "if df['Programme'].dtype == 'int64' or df['Programme'].iloc[0] in [1, 2, 3, 4]:\n",
    "    df['Programme'] = df['Programme'].map(mapping)\n",
    "\n",
    "# 数据处理和特征提取函数\n",
    "def process_data(df, mode='train', preprocessors=None):\n",
    "    \"\"\"\n",
    "    处理数据并提取特征，确保训练和测试数据使用相同的预处理\n",
    "\n",
    "    参数:\n",
    "    df - 输入数据框\n",
    "    mode - 'train'或'test'\n",
    "    preprocessors - 训练模式下创建并返回，测试模式下使用\n",
    "\n",
    "    返回:\n",
    "    feature_sets - 特征集字典\n",
    "    preprocessors - 预处理器字典(仅训练模式)\n",
    "    \"\"\"\n",
    "    # 去除索引列（如果存在）\n",
    "    if 'Index' in df.columns:\n",
    "        df = df.drop('Index', axis=1)\n",
    "\n",
    "    print(f\"{mode}数据集形状: {df.shape}\")\n",
    "    print(f\"\\n{mode}数据集前5行:\")\n",
    "    print(df.head())\n",
    "\n",
    "    # 只保留数值特征\n",
    "    numeric_df = df.select_dtypes(include=['float64', 'int64'])\n",
    "\n",
    "    feature_sets = {}\n",
    "\n",
    "    if mode == 'train':\n",
    "        preprocessors = {'scalers': {}}\n",
    "\n",
    "    # 特征集1：考试分数\n",
    "    if any('Q' in col for col in numeric_df.columns):\n",
    "        exam_cols = [col for col in numeric_df.columns if 'Q' in col]\n",
    "        if mode == 'train':\n",
    "            preprocessors['exam_cols'] = exam_cols\n",
    "    else:\n",
    "        if mode == 'train':\n",
    "            # 使用最后5列\n",
    "            exam_cols = numeric_df.columns[-5:].tolist()\n",
    "            preprocessors['exam_cols'] = exam_cols\n",
    "        else:\n",
    "            # 测试模式使用保存的列名\n",
    "            exam_cols = preprocessors['exam_cols']\n",
    "\n",
    "    # 确保测试数据有相同的列\n",
    "    available_exam_cols = [col for col in exam_cols if col in numeric_df.columns]\n",
    "    if len(available_exam_cols) != len(exam_cols):\n",
    "        print(f\"警告: 测试数据缺少一些考试分数列，使用可用的{len(available_exam_cols)}列\")\n",
    "\n",
    "    feature_sets['考试分数'] = numeric_df[available_exam_cols].values\n",
    "\n",
    "    # 特征集2：学生基本信息，去除年级\n",
    "    basic_patterns = ['性别', 'Gender', 'sex', 'Total', '总分', 'Q1', 'Q2', 'Q3', 'Q4', 'Q5']\n",
    "    if mode == 'train':\n",
    "        basic_cols = []\n",
    "        for pat in basic_patterns:\n",
    "            basic_cols += [col for col in numeric_df.columns if pat.lower() in col.lower()]\n",
    "        basic_cols = list(dict.fromkeys(basic_cols))  # 去重\n",
    "        preprocessors['basic_cols'] = basic_cols\n",
    "\n",
    "        if not basic_cols:\n",
    "            basic_cols = numeric_df.columns[:2].tolist()\n",
    "            preprocessors['basic_cols'] = basic_cols\n",
    "    else:\n",
    "        # 测试模式使用保存的列名\n",
    "        basic_cols = preprocessors['basic_cols']\n",
    "\n",
    "    # 确保测试数据有相同的列\n",
    "    available_basic_cols = [col for col in basic_cols if col in numeric_df.columns]\n",
    "    feature_sets['去除年级'] = numeric_df[available_basic_cols].values\n",
    "\n",
    "    # 特征集3：全部特征（排除programme列）\n",
    "    if mode == 'train':\n",
    "        programme_cols = [col for col in numeric_df.columns if 'programme' in col.lower() or 'program' in col.lower()]\n",
    "        preprocessors['programme_cols'] = programme_cols\n",
    "    else:\n",
    "        programme_cols = preprocessors['programme_cols']\n",
    "\n",
    "    if programme_cols:\n",
    "        exclude_cols = [col for col in programme_cols if col in numeric_df.columns]\n",
    "        feature_sets['全部特征'] = numeric_df.drop(columns=exclude_cols).values\n",
    "    else:\n",
    "        feature_sets['全部特征'] = numeric_df.values\n",
    "\n",
    "    # 标准化处理\n",
    "    for name, data in feature_sets.items():\n",
    "        if mode == 'train':\n",
    "            # 训练模式：创建并拟合标准化器\n",
    "            scaler = StandardScaler()\n",
    "            feature_sets[name] = scaler.fit_transform(data)\n",
    "            preprocessors['scalers'][name] = scaler\n",
    "        else:\n",
    "            # 测试模式：使用已拟合的标准化器\n",
    "            if name in preprocessors['scalers']:\n",
    "                feature_sets[name] = preprocessors['scalers'][name].transform(data)\n",
    "            else:\n",
    "                print(f\"错误: 没有找到特征集{name}的预处理器\")\n",
    "\n",
    "    if mode == 'train':\n",
    "        return feature_sets, preprocessors\n",
    "    else:\n",
    "        return feature_sets\n",
    "\n",
    "# 训练阶段\n",
    "\n",
    "\n",
    "# 测试阶段(示例) - 预测时使用\n",
    "# test_feature_sets = process_data(test_df, mode='test', preprocessors=preprocessors)\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "导入数据后，查看数据集的基本信息。处理数据缺失\n",
    "## 特征转换\n",
    "应用三种不同的数据转换方法：标准化缩放、PCA降维和独立成分分析(ICA)"
   ],
   "id": "463a157097939261"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "# 去除索引列（如果存在）\n",
    "if 'Index' in df.columns:\n",
    "    df = df.drop('Index', axis=1)\n",
    "\n",
    "print(f\"数据集形状: {df.shape}\")\n",
    "print(\"\\n数据集前5行:\")\n",
    "display(df.head())\n",
    "print(\"\\n数据集信息:\")\n",
    "display(df.info())\n",
    "print(f\"\\n缺失值情况:\")\n",
    "display(df.isnull().sum())\n",
    "\n",
    "# 只保留数值特征\n",
    "numeric_df = df.select_dtypes(include=['float64', 'int64'])\n",
    "feature_sets, preprocessors = process_data(df, mode='train')\n",
    "\n",
    "print(\"创建的特征集:\")\n",
    "for name, features in feature_sets.items():\n",
    "    print(f\"- {name}: 形状 {features.shape}\")\n",
    "\n",
    "# 检查特征集并标准化\n",
    "for name, data in feature_sets.items():\n",
    "    print(f\"\\n特征集: {name}\")\n",
    "    print(f\"数据集形状: {data.shape}\")\n",
    "    print(\"前5行:\")\n",
    "    print(pd.DataFrame(data[:5]))\n",
    "    # 标准化\n",
    "    scaler = StandardScaler()\n",
    "    feature_sets[name] = scaler.fit_transform(data)\n",
    "print(\"\\n数据集统计描述:\")\n",
    "display(df.describe())"
   ],
   "id": "1cd20b2f5cc4bf8d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 特征转换\n",
    "应用三种不同的数据转换方法：标准化缩放、PCA降维和独立成分分析(ICA)"
   ],
   "id": "80e120ce89748023"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "feature_set = feature_sets\n",
    "feature_sets = {}\n",
    "for name, X in feature_set.items():\n",
    "    # 转换1: 归一化 (MinMaxScaler)\n",
    "    min_max_scaler = MinMaxScaler()\n",
    "    X_minmax = min_max_scaler.fit_transform(X)\n",
    "    feature_sets['归一化_' + name] = X_minmax\n",
    "    print(f\"{name} 已完成归一化转换\")\n",
    "\n",
    "    # 转换2: 标准化 (StandardScaler)\n",
    "    std_scaler = StandardScaler()\n",
    "    X_std = std_scaler.fit_transform(X)\n",
    "    feature_sets['标准化_' + name] = X_std\n",
    "    print(f\"{name} 已完成标准化转换\")\n",
    "\n",
    "    # 转换3: 鲁棒缩放 (RobustScaler，对异常值不敏感)\n",
    "    min_max_scaler = MinMaxScaler()\n",
    "    X_minmax = min_max_scaler.fit_transform(X)\n",
    "    feature_sets['MinMax缩放_' + name] = X_minmax\n",
    "    print(f\"{name} 已完成MinMax缩放转换\")\n",
    "\n",
    "feature_set = feature_sets\n",
    "feature_sets = {}\n",
    "\n",
    "for name, X_scaled in feature_set.items():\n",
    "    # 转换2: PCA降维\n",
    "    pca = PCA(n_components=min(X_scaled.shape[1], 10))\n",
    "    X_pca = pca.fit_transform(X_scaled)\n",
    "    feature_sets['PCA_' + name] = X_pca\n",
    "    print(f\"{name} PCA解释方差比: {pca.explained_variance_ratio_}\")\n",
    "    print(f\"{name} PCA累计方差占比: {np.sum(pca.explained_variance_ratio_):.4f}\")\n",
    "\n",
    "    # 转换3: FastICA\n",
    "    ica = FastICA(n_components=min(X_scaled.shape[1], 10), random_state=42)\n",
    "    X_ica = ica.fit_transform(X_scaled)\n",
    "    feature_sets['ICA_' + name] = X_ica\n",
    "    print(f\"{name} 已完成ICA转换\")\n",
    "    # 转换4: t-SNE\n",
    "    tsne = TSNE(n_components=2, random_state=42, init='random', learning_rate='auto')\n",
    "    X_tsne = tsne.fit_transform(X_scaled)\n",
    "    feature_sets['TSNE_' + name] = X_tsne\n",
    "    print(f\"{name} 已完成t-SNE转换\")\n",
    "\n",
    "# 可视化所有特征集\n",
    "n = len(feature_sets)\n",
    "cols = 3\n",
    "rows = math.ceil(n / cols)\n",
    "plt.figure(figsize=(5 * cols, 5 * rows))\n",
    "for i, (fname, data) in enumerate(feature_sets.items()):\n",
    "    plt.subplot(rows, cols, i + 1)\n",
    "    if data.shape[1] > 1:\n",
    "        plt.scatter(data[:, 0], data[:, 1], alpha=0.5)\n",
    "        plt.title(f\"{fname} (前两个维度)\")\n",
    "    else:\n",
    "        plt.hist(data[:, 0], bins=20)\n",
    "        plt.title(f\"{fname}分布\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "58371bedbe221148",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 聚类评估函数\n",
    "定义用于评估聚类结果的性能指标函数"
   ],
   "id": "1e252efeb4b9f3f6"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 第四个代码单元格 - 评估函数\n",
    "def evaluate_clustering(X, labels, name):\n",
    "    \"\"\"计算聚类性能指标\"\"\"\n",
    "    try:\n",
    "        silhouette = silhouette_score(X, labels)\n",
    "    except:\n",
    "        silhouette = -1\n",
    "\n",
    "    try:\n",
    "        db_score = davies_bouldin_score(X, labels)\n",
    "    except:\n",
    "        db_score = float('inf')\n",
    "\n",
    "    try:\n",
    "        ch_score = calinski_harabasz_score(X, labels)\n",
    "    except:\n",
    "        ch_score = -1\n",
    "\n",
    "    return {\n",
    "        'silhouette_score': silhouette,  # 越高越好\n",
    "        'davies_bouldin_score': db_score,  # 越低越好\n",
    "        'calinski_harabasz_score': ch_score,  # 越高越好\n",
    "        'method': name\n",
    "    }"
   ],
   "id": "22122caea0e5c02b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 聚类算法实现\n",
    "实现三种聚类算法及其不同参数设置：\n",
    "1. K-means聚类\n",
    "2. 高斯混合模型(GMM)\n",
    "3. 层次聚类(Hierarchical Clustering)"
   ],
   "id": "912161e51de0a99"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 第五个代码单元格 - K-means聚类\n",
    "def run_kmeans(X, n_clusters_range=[4], init_methods=['k-means++', 'random']):\n",
    "    \"\"\"运行K-means并尝试不同参数\"\"\"\n",
    "    results = []\n",
    "\n",
    "    for n_clusters in n_clusters_range:\n",
    "        for init in init_methods:\n",
    "            name = f\"KMeans(n_clusters={n_clusters}, init={init})\"\n",
    "            try:\n",
    "                model = KMeans(n_clusters=n_clusters, init=init, random_state=42)\n",
    "                labels = model.fit_predict(X)\n",
    "\n",
    "                # 评估结果\n",
    "                result = evaluate_clustering(X, labels, name)\n",
    "                result['labels'] = labels\n",
    "                result['model'] = model\n",
    "                result['n_clusters'] = n_clusters\n",
    "                result['init'] = init\n",
    "\n",
    "                results.append(result)\n",
    "                print(f\"完成: {name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"错误 {name}: {str(e)}\")\n",
    "\n",
    "    return results"
   ],
   "id": "fb3cd5861694686f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 第六个代码单元格 - 高斯混合模型\n",
    "def run_gmm(X, n_components_range=[2, 3, 4, 5, 6, 7, 8], covariance_types=['full', 'tied', 'diag', 'spherical']):\n",
    "    \"\"\"运行高斯混合模型并尝试不同参数\"\"\"\n",
    "    results = []\n",
    "\n",
    "    for n_components in n_components_range:\n",
    "        for cov_type in covariance_types:\n",
    "            name = f\"GMM(n_components={n_components}, covariance_type={cov_type})\"\n",
    "            try:\n",
    "                model = GaussianMixture(n_components=n_components, covariance_type=cov_type, random_state=42)\n",
    "                labels = model.fit_predict(X)\n",
    "\n",
    "                # 评估结果\n",
    "                result = evaluate_clustering(X, labels, name)\n",
    "                result['labels'] = labels\n",
    "                result['model'] = model\n",
    "                result['n_components'] = n_components\n",
    "                result['covariance_type'] = cov_type\n",
    "\n",
    "                results.append(result)\n",
    "                print(f\"完成: {name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"错误 {name}: {str(e)}\")\n",
    "\n",
    "    return results"
   ],
   "id": "810332aa41d28560",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 第七个代码单元格 - 层次聚类\n",
    "def run_hierarchical(X, n_clusters_range=[4], linkage_methods=['ward', 'complete', 'average', 'single']):\n",
    "    \"\"\"运行层次聚类并尝试不同参数\"\"\"\n",
    "    results = []\n",
    "\n",
    "    for n_clusters in n_clusters_range:\n",
    "        for linkage in linkage_methods:\n",
    "            # ward只能用于欧氏距离\n",
    "            if linkage == 'ward':\n",
    "                affinity = 'euclidean'\n",
    "            else:\n",
    "                affinity = 'euclidean'  # 也可以尝试其他距离\n",
    "\n",
    "            name = f\"HC(n_clusters={n_clusters}, linkage={linkage})\"\n",
    "            try:\n",
    "                model = AgglomerativeClustering(n_clusters=n_clusters, linkage=linkage, affinity=affinity)\n",
    "                labels = model.fit_predict(X)\n",
    "\n",
    "                # 评估结果\n",
    "                result = evaluate_clustering(X, labels, name)\n",
    "                result['labels'] = labels\n",
    "                result['model'] = model\n",
    "                result['n_clusters'] = n_clusters\n",
    "                result['linkage'] = linkage\n",
    "\n",
    "                results.append(result)\n",
    "                print(f\"完成: {name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"错误 {name}: {str(e)}\")\n",
    "\n",
    "    return results"
   ],
   "id": "dd71590f41beeb47",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 运行聚类实验\n",
    "对每种特征集运行三种聚类算法，并尝试不同的参数设置"
   ],
   "id": "e1ae4fa3230ca7aa"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 第八个代码单元格 - 运行所有实验\n",
    "# 为了限制运行时间，可以减少参数组合\n",
    "n_clusters_range = [4]  # 聚类数量范围\n",
    "init_methods =['k-means++']  # K-means初始化方法\n",
    "covariance_types = ['full', 'tied']  # GMM协方差类型\n",
    "linkage_methods = ['ward', 'complete']  # 层次聚类链接方法\n",
    "\n",
    "# 4. 运行所有实验并收集结果\n",
    "all_results = {}\n",
    "\n",
    "for feature_name, X_transformed in feature_sets.items():\n",
    "    print(f\"\\n处理特征集: {feature_name}\")\n",
    "\n",
    "    # 运行三种聚类算法\n",
    "    kmeans_results = run_kmeans(X_transformed, n_clusters_range, init_methods)\n",
    "    gmm_results = run_gmm(X_transformed, n_clusters_range, covariance_types)\n",
    "    hc_results = run_hierarchical(X_transformed, n_clusters_range, linkage_methods)\n",
    "\n",
    "    # 保存结果\n",
    "    all_results[feature_name] = {\n",
    "        'kmeans': kmeans_results,\n",
    "        'gmm': gmm_results,\n",
    "        'hierarchical': hc_results\n",
    "    }"
   ],
   "id": "2135d667c47b14a4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 结果分析\n",
    "找出每种特征集和每种聚类方法的最佳结果"
   ],
   "id": "52058251351e3093"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 第九个代码单元格 - 找出最佳结果\n",
    "# 5. 找出每种特征集和每种聚类方法的最佳结果\n",
    "best_results = {}\n",
    "\n",
    "for feature_name, methods in all_results.items():\n",
    "    best_results[feature_name] = {}\n",
    "\n",
    "    for method_name, results in methods.items():\n",
    "        if method_name in ['kmeans', 'gmm', 'hierarchical']:\n",
    "            # 按silhouette_score排序（越高越好）\n",
    "            sorted_results = sorted(results, key=lambda x: x['silhouette_score'], reverse=True)\n",
    "            if sorted_results:\n",
    "                best_results[feature_name][method_name] = sorted_results[0]"
   ],
   "id": "331186d01fb8850b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 第十个代码单元格 - 创建结果表格\n",
    "# 6. 创建结果表格\n",
    "results_table = []\n",
    "\n",
    "for feature_name, methods in best_results.items():\n",
    "    for method_name, result in methods.items():\n",
    "        row = {\n",
    "            '特征集': feature_name,\n",
    "            '聚类方法': method_name,\n",
    "            '轮廓系数': result['silhouette_score'],\n",
    "            'Davies-Bouldin': result['davies_bouldin_score'],\n",
    "            'Calinski-Harabasz': result['calinski_harabasz_score']\n",
    "        }\n",
    "\n",
    "        # 添加模型特有的参数\n",
    "        if method_name == 'kmeans':\n",
    "            row['聚类数'] = result['n_clusters']\n",
    "            row['初始化方法'] = result['init']\n",
    "        elif method_name == 'gmm':\n",
    "            row['聚类数/组件数'] = result['n_components']\n",
    "            row['协方差类型'] = result['covariance_type']\n",
    "        elif method_name == 'hierarchical':\n",
    "            row['聚类数'] = result['n_clusters']\n",
    "            row['链接方法'] = result['linkage']\n",
    "\n",
    "        results_table.append(row)\n",
    "\n",
    "# 7. 将结果转换为DataFrame并显示\n",
    "results_df = pd.DataFrame(results_table)\n",
    "print(\"\\n聚类结果表:\")\n",
    "display(results_df)"
   ],
   "id": "d0dc20f3b3c904bd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 可视化聚类结果\n",
    "可视化展示每种特征集和聚类方法的最佳聚类结果"
   ],
   "id": "cc05aed3c300399a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 第十一个代码单元格 - 可视化最佳结果\n",
    "# 8. 可视化每种特征集和聚类方法的最佳结果\n",
    "for feature_name, methods in best_results.items():\n",
    "    for method_name, result in methods.items():\n",
    "        # 如果特征维度大于2，使用PCA降至2维进行可视化\n",
    "        if feature_sets[feature_name].shape[1] > 2:\n",
    "            vis_pca = PCA(n_components=2)\n",
    "            X_vis = vis_pca.fit_transform(feature_sets[feature_name])\n",
    "        else:\n",
    "            X_vis = feature_sets[feature_name]\n",
    "\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.scatter(X_vis[:, 0], X_vis[:, 1], c=result['labels'], cmap='viridis', alpha=0.8, s=50)\n",
    "        plt.title(f'特征集: {feature_name}, 聚类方法: {method_name}\\n轮廓系数: {result[\"silhouette_score\"]:.4f}')\n",
    "        plt.colorbar(label='聚类标签')\n",
    "        plt.tight_layout()\n",
    "        plt.show()"
   ],
   "id": "40f5e316f7a0b8b3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# 第十二个代码单元格 - 汇总表格\n",
    "# 9. 生成最佳结果的汇总表格\n",
    "summary_df = pd.DataFrame(results_table)\n",
    "\n",
    "# 按特征集和聚类方法分组，找出每种组合的最佳结果\n",
    "best_by_feature = summary_df.sort_values('轮廓系数', ascending=False).groupby(['特征集', '聚类方法']).first().reset_index()\n",
    "\n",
    "print(\"最佳聚类结果汇总表:\")\n",
    "display(best_by_feature)\n",
    "\n",
    "# 创建热力图显示不同特征集和聚类方法的轮廓系数\n",
    "pivot_table = best_by_feature.pivot(index='特征集', columns='聚类方法', values='轮廓系数')\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(pivot_table, annot=True, cmap='YlGnBu', fmt='.4f')\n",
    "plt.title('不同特征集和聚类方法的轮廓系数')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ],
   "id": "5398d57f7fec47cf",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import warnings\n",
    "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score, confusion_matrix\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# 真实标签编码，只需一次\n",
    "y_true = LabelEncoder().fit_transform(df['Programme'])\n",
    "\n",
    "# 结果表格\n",
    "results_table = []\n",
    "\n",
    "for feature_name, methods in best_results.items():\n",
    "    for method_name, result in methods.items():\n",
    "        # 记录聚类评估指标\n",
    "        row = {\n",
    "            '特征集': feature_name,\n",
    "            '聚类方法': method_name,\n",
    "            '轮廓系数': result['silhouette_score'],\n",
    "            'Davies-Bouldin': result['davies_bouldin_score'],\n",
    "            'Calinski-Harabasz': result['calinski_harabasz_score']\n",
    "        }\n",
    "        if method_name == 'kmeans':\n",
    "            row['聚类数'] = result['n_clusters']\n",
    "            row['初始化方法'] = result['init']\n",
    "        elif method_name == 'gmm':\n",
    "            row['组件数'] = result['n_components']\n",
    "            row['协方差类型'] = result['covariance_type']\n",
    "        elif method_name == 'hierarchical':\n",
    "            row['聚类数'] = result['n_clusters']\n",
    "            row['链接方法'] = result['linkage']\n",
    "        results_table.append(row)\n",
    "\n",
    "        # 可视化\n",
    "        X_plot = feature_sets[feature_name]\n",
    "        if X_plot.shape[1] > 2:\n",
    "            X_vis = PCA(n_components=2).fit_transform(X_plot)\n",
    "        else:\n",
    "            X_vis = X_plot\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        plt.scatter(X_vis[:, 0], X_vis[:, 1], c=result['labels'], cmap='viridis', alpha=0.8, s=50)\n",
    "        plt.title(f'特征集: {feature_name}, 聚类方法: {method_name}\\n轮廓系数: {result[\"silhouette_score\"]:.4f}')\n",
    "        plt.colorbar(label='聚类标签')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # 聚类与真实标签对比\n",
    "        labels = result['labels']\n",
    "        ari = adjusted_rand_score(y_true, labels)\n",
    "        nmi = normalized_mutual_info_score(y_true, labels)\n",
    "        print(f\"\\n特征集: {feature_name}, 聚类方法: {method_name}\")\n",
    "        print(f\"调整兰德指数(ARI): {ari:.4f}\")\n",
    "        print(f\"归一化互信息(NMI): {nmi:.4f}\")\n",
    "        print(\"混淆矩阵:\")\n",
    "        print(confusion_matrix(y_true, labels))\n",
    "\n",
    "# 结果表格展示\n",
    "results_df = pd.DataFrame(results_table)\n",
    "print(\"\\n聚类结果表:\")\n",
    "print(results_df)"
   ],
   "id": "c9d7bda69195b35e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from preproduce import best_model\n",
    "# 导入必要的库\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix, adjusted_rand_score\n",
    "from evaluate import evaluate_clustering\n",
    "\n",
    "# 1. 读取测试数据\n",
    "test_df = pd.read_csv('test_data.csv')\n",
    "print(f\"测试数据形状: {test_df.shape}\")\n",
    "\n",
    "# 2. 确保测试特征维度与训练特征一致\n",
    "X_test = test_df.select_dtypes(include=['float64', 'int64']).values\n",
    "print(f\"原始测试特征维度: {X_test.shape}\")\n",
    "\n",
    "# 获取最佳模型的特征数量\n",
    "n_features_expected = best_model.n_features_in_  # 这是模型期望的特征数\n",
    "print(f\"模型期望的特征数量: {n_features_expected}\")\n",
    "\n",
    "# 调整测试数据的维度\n",
    "if X_test.shape[1] > n_features_expected:\n",
    "    X_test = X_test[:, :n_features_expected]  # 如果特征过多，截取前n个\n",
    "elif X_test.shape[1] < n_features_expected:\n",
    "    # 如果特征不足，用零填充\n",
    "    padding = np.zeros((X_test.shape[0], n_features_expected - X_test.shape[1]))\n",
    "    X_test = np.hstack((X_test, padding))\n",
    "\n",
    "print(f\"调整后测试特征维度: {X_test.shape}\")\n",
    "\n",
    "# 3. 标准化处理\n",
    "scaler = StandardScaler()\n",
    "X_test_scaled = scaler.fit_transform(X_test)\n",
    "\n",
    "# 4. 使用模型预测\n",
    "test_labels = best_model.predict(X_test_scaled)\n",
    "print(f\"测试数据聚类完成，标签分布: {np.unique(test_labels, return_counts=True)}\")\n",
    "\n",
    "# 5. 使用evaluate_clustering函数评估\n",
    "ratio = evaluate_clustering(X=X_test_scaled, labels=test_labels)\n",
    "print(f\"簇内距离与簇间距离比率: {ratio:.4f}\")"
   ],
   "id": "f13bcab11e7f00bd",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
