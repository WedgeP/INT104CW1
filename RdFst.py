import platform
import pandas as pd
import numpy as np
import warnings

from sklearn.preprocessing import StandardScaler, MinMaxScaler, Normalizer
from sklearn.decomposition import PCA, FastICA
from sklearn.manifold import TSNE
from sklearn.cluster import KMeans, AgglomerativeClustering
from sklearn.mixture import GaussianMixture
from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score

# è®¾ç½®éšæœºç§å­
np.random.seed(42)

# è¯»å–æ•°æ®
df = pd.read_csv('./student_data.csv')

# å¤„ç† Programme å­—æ®µ
mapping = {1: 'A', 2: 'B', 3: 'C', 4: 'D'}
if df['Programme'].dtype == 'int64' or df['Programme'].iloc[0] in [1, 2, 3, 4]:
    df['Programme'] = df['Programme'].map(mapping)

# ç‰¹å¾å·¥ç¨‹
def process_data(df, mode='train', preprocessors=None):
    if 'Index' in df.columns:
        df = df.drop('Index', axis=1)
    numeric_df = df.select_dtypes(include=['float64', 'int64'])
    feature_sets = {}

    if mode == 'train':
        preprocessors = {'scalers': {}, 'columns': {}}

    if mode == 'train':
        exam_cols = [col for col in numeric_df.columns if 'Q' in col]
        if not exam_cols:
            exam_cols = numeric_df.columns[-5:].tolist()
        preprocessors['columns']['è€ƒè¯•åˆ†æ•°'] = exam_cols
    else:
        exam_cols = preprocessors['columns']['è€ƒè¯•åˆ†æ•°']
    for col in exam_cols:
        if col not in numeric_df.columns:
            numeric_df[col] = 0
    feature_sets['è€ƒè¯•åˆ†æ•°'] = numeric_df[exam_cols].values

    basic_patterns = ['æ€§åˆ«', 'Gender', 'sex', 'Total', 'æ€»åˆ†', 'Q1', 'Q2', 'Q3', 'Q4', 'Q5']
    if mode == 'train':
        basic_cols = []
        for pat in basic_patterns:
            basic_cols += [col for col in numeric_df.columns if pat.lower() in col.lower()]
        basic_cols = list(dict.fromkeys(basic_cols))
        if not basic_cols:
            basic_cols = numeric_df.columns[:2].tolist()
        preprocessors['columns']['å»é™¤å¹´çº§'] = basic_cols
    else:
        basic_cols = preprocessors['columns']['å»é™¤å¹´çº§']
    for col in basic_cols:
        if col not in numeric_df.columns:
            numeric_df[col] = 0
    feature_sets['å»é™¤å¹´çº§'] = numeric_df[basic_cols].values

    if mode == 'train':
        programme_cols = [col for col in numeric_df.columns if 'programme' in col.lower() or 'program' in col.lower()]
        all_cols = [col for col in numeric_df.columns if col not in programme_cols]
        preprocessors['columns']['å…¨éƒ¨ç‰¹å¾'] = all_cols
    else:
        all_cols = preprocessors['columns']['å…¨éƒ¨ç‰¹å¾']
    for col in all_cols:
        if col not in numeric_df.columns:
            numeric_df[col] = 0
    feature_sets['å…¨éƒ¨ç‰¹å¾'] = numeric_df[all_cols].values

    for name, data in feature_sets.items():
        if mode == 'train':
            scaler = StandardScaler()
            feature_sets[name] = scaler.fit_transform(data)
            preprocessors['scalers'][name] = scaler
        else:
            feature_sets[name] = preprocessors['scalers'][name].transform(data)

    if mode == 'train':
        return feature_sets, preprocessors
    else:
        return feature_sets

feature_sets, preprocessors = process_data(df, mode='train')

# ç‰¹å¾è½¬æ¢
transformed_sets = {}
for name, X in feature_sets.items():
    minmax = MinMaxScaler().fit_transform(X)
    transformed_sets['å½’ä¸€åŒ–_' + name] = minmax

    standard = StandardScaler().fit_transform(X)
    transformed_sets['æ ‡å‡†åŒ–_' + name] = standard

    normalized = Normalizer().fit_transform(X)
    transformed_sets['æ­£åˆ™ç¼©æ”¾_' + name] = normalized

# é™ç»´å¤„ç†
final_sets = {}
for name, X_scaled in transformed_sets.items():
    pca = PCA(n_components=min(X_scaled.shape[1], 10))
    final_sets['PCA_' + name] = pca.fit_transform(X_scaled)

    ica = FastICA(n_components=min(X_scaled.shape[1], 10), random_state=42)
    final_sets['ICA_' + name] = ica.fit_transform(X_scaled)

    tsne = TSNE(n_components=2, random_state=42, init='random', learning_rate='auto')
    final_sets['TSNE_' + name] = tsne.fit_transform(X_scaled)

# èšç±»è¯„ä¼°
def evaluate_clustering(X, labels):
    try:
        silhouette = silhouette_score(X, labels)
    except:
        silhouette = -1
    try:
        db_score = davies_bouldin_score(X, labels)
    except:
        db_score = float('inf')
    try:
        ch_score = calinski_harabasz_score(X, labels)
    except:
        ch_score = -1
    return silhouette, db_score, ch_score

def run_kmeans(X, n_clusters=[4]):
    results = []
    for n in n_clusters:
        model = KMeans(n_clusters=n, init='k-means++', random_state=42)
        labels = model.fit_predict(X)
        silhouette, db, ch = evaluate_clustering(X, labels)
        results.append({'method': 'kmeans', 'n_clusters': n, 'silhouette': silhouette, 'db': db, 'ch': ch})
    return results

def run_gmm(X, n_components=[4]):
    results = []
    for n in n_components:
        model = GaussianMixture(n_components=n, covariance_type='full', random_state=42)
        labels = model.fit_predict(X)
        silhouette, db, ch = evaluate_clustering(X, labels)
        results.append({'method': 'gmm', 'n_components': n, 'silhouette': silhouette, 'db': db, 'ch': ch})
    return results

def run_hierarchical(X, n_clusters=[4]):
    results = []
    for n in n_clusters:
        model = AgglomerativeClustering(n_clusters=n, linkage='ward', metric='euclidean')
        labels = model.fit_predict(X)
        silhouette, db, ch = evaluate_clustering(X, labels)
        results.append({'method': 'hierarchical', 'n_clusters': n, 'silhouette': silhouette, 'db': db, 'ch': ch})
    return results

# è¿è¡Œæ‰€æœ‰å®éªŒ
all_results = {}
for feature_name, X in final_sets.items():
    kmeans_res = run_kmeans(X)
    gmm_res = run_gmm(X)
    hc_res = run_hierarchical(X)
    all_results[feature_name] = kmeans_res + gmm_res + hc_res

# æå–æœ€ä½³ç»“æœ
best_results = {}
for feature_name, results in all_results.items():
    best = max(results, key=lambda x: x['silhouette'])
    best_results[feature_name] = best

from evaluate import evaluate_clustering

# 1. å®šä¹‰å¤„ç†å‡½æ•°ï¼ˆç”¨ä¹‹å‰çš„preprocessorsï¼‰
def process_test_data(df_test, preprocessors):
    if 'Index' in df_test.columns:
        df_test = df_test.drop('Index', axis=1)
    numeric_df = df_test.select_dtypes(include=['float64', 'int64'])
    feature_sets = {}

    exam_cols = preprocessors['columns']['è€ƒè¯•åˆ†æ•°']
    for col in exam_cols:
        if col not in numeric_df.columns:
            numeric_df[col] = 0
    feature_sets['è€ƒè¯•åˆ†æ•°'] = numeric_df[exam_cols].values

    basic_cols = preprocessors['columns']['å»é™¤å¹´çº§']
    for col in basic_cols:
        if col not in numeric_df.columns:
            numeric_df[col] = 0
    feature_sets['å»é™¤å¹´çº§'] = numeric_df[basic_cols].values

    all_cols = preprocessors['columns']['å…¨éƒ¨ç‰¹å¾']
    for col in all_cols:
        if col not in numeric_df.columns:
            numeric_df[col] = 0
    feature_sets['å…¨éƒ¨ç‰¹å¾'] = numeric_df[all_cols].values

    for name, data in feature_sets.items():
        feature_sets[name] = preprocessors['scalers'][name].transform(data)

    return feature_sets

# 2. æ‰¾åˆ°è®­ç»ƒæ—¶æœ€å¥½çš„ç‰¹å¾+èšç±»æ–¹æ³•
# å‡è®¾ä½ å·²ç»æœ‰äº† best_results è¿™ä¸ªå­—å…¸
# æ¯”å¦‚ï¼š
# best_results = {'PCA_è€ƒè¯•åˆ†æ•°': {'method': 'kmeans', 'n_clusters': 4, 'silhouette': 0.4, ...}, ...}

# æ‰¾åˆ° silhouette score æœ€é«˜çš„é‚£ä¸€è¡Œ
best_feature_name, best_model_info = max(best_results.items(), key=lambda x: x[1]['silhouette'])

print(f"âœ… é€‰ä¸­çš„æœ€ä½³ç‰¹å¾å¤„ç†æ–¹å¼: {best_feature_name}")
print(f"âœ… é€‰ä¸­çš„æœ€ä½³èšç±»æ–¹æ³•: {best_model_info}")

# æå–æ¨¡å‹å‚æ•°
method = best_model_info['method']
n_clusters = best_model_info.get('n_clusters') or best_model_info.get('n_components')

# 3. é‡æ–°è®­ç»ƒæœ€ä½³æ¨¡å‹
X_train_best = final_sets[best_feature_name]

if method == 'kmeans':
    best_model = KMeans(n_clusters=n_clusters, init='k-means++', random_state=42)
elif method == 'gmm':
    best_model = GaussianMixture(n_components=n_clusters, covariance_type='full', random_state=42)
elif method == 'hierarchical':
    best_model = AgglomerativeClustering(n_clusters=n_clusters, linkage='ward', metric='euclidean')
else:
    raise ValueError("ä¸æ”¯æŒçš„èšç±»æ–¹æ³•ï¼")

# æ³¨æ„ï¼šAgglomerativeClusteringæ˜¯æ²¡æœ‰.fit_predictä¹‹å¤–çš„predictï¼Œéœ€è¦ç‰¹æ®Šå¤„ç†
if method == 'hierarchical':
    best_model.fit(X_train_best)
else:
    best_model.fit(X_train_best)

# 4. å¤„ç†æ–°çš„æµ‹è¯•é›† student_data.csv
df_test = pd.read_csv('./test_data.csv')

# å¦‚æœæœ‰Programmeåˆ—ä¸”æ˜¯æ•°å­—ï¼Œæ˜ å°„ä¸€ä¸‹
mapping = {1: 'A', 2: 'B', 3: 'C', 4: 'D'}
if 'Programme' in df_test.columns and (df_test['Programme'].dtype == 'int64' or df_test['Programme'].iloc[0] in [1,2,3,4]):
    df_test['Programme'] = df_test['Programme'].map(mapping)

# å¤„ç†ç‰¹å¾
test_feature_sets = process_test_data(df_test, preprocessors)

# æ‰¾åˆ°åŸå§‹ç‰¹å¾
base_name = best_feature_name.split('_')[-1]  # æ¯”å¦‚PCA_è€ƒè¯•åˆ†æ•° -> è€ƒè¯•åˆ†æ•°
X_test_base = test_feature_sets[base_name]

# æŒ‰ç‰¹å¾å¤„ç†ï¼ˆæ¯”å¦‚ PCAï¼‰
if best_feature_name.startswith('PCA'):
    pca = PCA(n_components=min(X_test_base.shape[1], 10))
    pca.fit(X_train_best)  # ç”¨è®­ç»ƒé›†æ¥fit PCA
    X_test_final = pca.transform(X_test_base)
elif best_feature_name.startswith('ICA'):
    ica = FastICA(n_components=min(X_test_base.shape[1], 10), random_state=42)
    ica.fit(X_train_best)
    X_test_final = ica.transform(X_test_base)
elif best_feature_name.startswith('TSNE'):
    tsne = TSNE(n_components=2, random_state=42, init='random', learning_rate='auto')
    tsne.fit(X_train_best)
    X_test_final = tsne.fit_transform(X_test_base)  # tsneæ²¡æ³•transformï¼Œåªèƒ½é‡æ–°fit
else:
    X_test_final = X_test_base

# 5. ç”Ÿæˆé¢„æµ‹labels
if method == 'hierarchical':
    predicted_labels = best_model.fit_predict(X_test_final)
else:
    predicted_labels = best_model.predict(X_test_final)

# 6. ä¿å­˜ç»“æœ
evaluate_clustering(X_test_final, predicted_labels, output_file='predicted_labels.csv')

print("ğŸ‰ æµ‹è¯•é›†çš„é¢„æµ‹ç»“æœå·²ç»ä¿å­˜åˆ° predicted_labels.csv")